from sklearn.model_selection import train_test_split
from torchvision import transforms
from .densenet import DenseNet
# from densenet import DenseNet
import torch
import os
import time
import numpy as np
from azureml.studio.core.logger import logger
from azureml.studio.core.io.model_directory import save_model_to_directory
from azureml.studio.core.utils.fileutils import ensure_folder


class AverageMeter(object):
    """
    Computes and stores the average and current value
    Copied from: https://github.com/pytorch/examples/blob/master/imagenet/main.py
    """
    def __init__(self):
        self.reset()

    def reset(self):
        self.val = 0
        self.avg = 0
        self.sum = 0
        self.count = 0

    def update(self, val, n=1):
        self.val = val
        self.sum += val * n
        self.count += n
        self.avg = self.sum / self.count


def load_model(model_path, model_type, memory_efficient, num_classes):
    model = DenseNet(model_type=model_type,
                     pretrained=False,
                     memory_efficient=memory_efficient,
                     classes=num_classes)
    model.load_state_dict(
        torch.load(os.path.join(model_path, 'best_model.pth'),
                   map_location='cpu'))
    if torch.cuda.is_available():
        model = model.cuda()
        if torch.cuda.device_count() > 1:
            model = torch.nn.DataParallel(model).cuda()
    return model


def torch_loader(load_from_dir, model_spec):
    """Load the pickle model by reading the file indicated by file_name in model_spec."""
    file_name = model_spec['file_name']
    with open(os.path.join(load_from_dir, file_name), 'rb') as fin:
        return pickle.load(fin)


def torch_dumper(state_dict, file_name=None):
    """Return a dumper to save torch state dict."""
    if not file_name:
        file_name = '_data.pth'

    def model_dumper(save_to):
        full_path = os.path.join(save_to, file_name)
        ensure_folder(os.path.dirname(os.path.abspath(full_path)))
        torch.save(state_dict, full_path)
        model_spec = {'model_type': 'torch_state_dict', 'file_name': file_name}
        return model_spec
    return model_dumper


def save_checkpoint(state, is_best, save_model_path, patience):
    # todo: save checkpoint files, but removed now to increase web service deployment efficiency
    # checkpoint_path = os.path.join(save_model_path, 'model_epoch_{}.pth'.format(state["epoch"]))
    # torch.save(state["state_dict"], checkpoint_path)
    logger.info(','.join([
            f'Epoch {state["epoch"]:d}',
            f'train_loss {state["train_loss"]:.6f}',
            f'train_error {state["train_error"]:.6f}',
            f'valid_loss {state["valid_loss"]:.5f}',
            f'valid_error {state["valid_error"]:.5f}\n'
        ]))
    best_checkpoint_name = 'best_model.pth'
    if is_best:
        dumper = torch_dumper(state["state_dict"], best_checkpoint_name)
        save_model_to_directory(save_model_path, dumper)
        logger.info(f'Get better top1 accuracy: {state["best_accuracy"]:.4f} saving weights to {best_checkpoint_name}')
        # shutil.copyfile(checkpoint_path, best_checkpoint_path)

    early_stop = True if state["counter"] >= patience else False
    if early_stop:
        logger.info("early stopped.")

    return early_stop


def get_transform():
    # mean and stdv of imagenet dataset.
    # Usually if you use case in the same data domain as imagenet,
    # the mean and std wonâ€™t be that different and you can try to use the ImageNet statistics.
    # todo: If you are dealing with another domain, e.g. medical images,
    # re-calculate stats on your own data is recommended.
    mean = [0.485, 0.456, 0.406]
    stdv = [0.229, 0.224, 0.225]
    train_transforms = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.RandomHorizontalFlip(),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=stdv),
    ])
    test_transforms = transforms.Compose([
        transforms.Resize(256),
        transforms.CenterCrop(224),
        transforms.ToTensor(),
        transforms.Normalize(mean=mean, std=stdv),
    ])
    return train_transforms, test_transforms


def evaluate(model, loader, print_freq=1, is_test=False):
    batch_time = AverageMeter()
    losses = AverageMeter()
    error = AverageMeter()
    # Model on eval mode
    model.eval()
    end = time.time()
    target_list = []
    pred_top1_list = []
    with torch.no_grad():
        for batch_idx, (input, target) in enumerate(loader):
            # Create variables
            if torch.cuda.is_available():
                input = input.cuda()
                target = target.cuda()
            # compute output
            output = model(input)
            loss = torch.nn.functional.cross_entropy(output, target)
            # measure accuracy and record loss
            batch_size = target.size(0)
            _, pred = output.data.cpu().topk(1, dim=1)
            target_list += target.tolist()
            pred_top1_list += [i[0] for i in pred.tolist()]
            error.update(
                torch.ne(pred.squeeze(), target.cpu()).float().sum().item() /
                batch_size, batch_size)
            losses.update(loss.item() / batch_size, batch_size)
            # measure elapsed time
            batch_time.update(time.time() - end)
            end = time.time()
            # print stats
            if batch_idx % print_freq == 0:
                res = '\t'.join([
                    'Test' if is_test else 'Valid',
                    'Iter: [{:d}/{:d}]'.format(batch_idx + 1, len(loader)),
                    'Avg_Time_Batch/Avg_Time_Epoch: {:.3f}/{:.3f}'.format(
                        batch_time.val, batch_time.avg),
                    'Avg_Loss_Batch/Avg_Loss_Epoch: {:.4f}/{:.4f}'.format(
                        losses.val, losses.avg),
                    'Avg_Error_Batch/Avg_Error_Epoch: {:.4f}/{:.4f}'.format(
                        error.val, error.avg),
                ])
                logger.info(res)
    # Return summary statistics
    return batch_time.avg, losses.avg, error.avg, (target_list, pred_top1_list)


def get_stratified_split_index(n_samples, class_idx_list, valid_size):
    sample_ids = np.array(list(range(n_samples)))
    labels = np.array(class_idx_list)
    train_index, valid_index, train_label, valid_label = train_test_split(
        sample_ids, labels, stratify=labels, test_size=valid_size)
    return train_index, valid_index
